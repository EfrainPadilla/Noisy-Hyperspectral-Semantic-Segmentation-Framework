# Noisy-Hyperspectral-Semantic-Segmentation-Framework
Code of the Publication "A Semantic Segmentation Framework based on Tucker Decomposition and Deep Learning for Noisy Scenarios of Hyperspectral Imagery" (Will be published soon)
Authors: Efrain Padilla-Zepeda, Deni Torres-Roman, Andres Mendez-Vazquez

The implementation of Deep Learning Models is based on the code of @mhaut https://github.com/mhaut/hyperspectral_deeplearning_review

The Tucker decomposition is a modification of a the Tensorly implementation of this algorithm. http://tensorly.org/stable/index.html

VBMF (Variational Bayes Matrix Factorization) script is not used for the content of the paper, but it is included for future work, used for rank-3 selection for Tucker decomposition.
